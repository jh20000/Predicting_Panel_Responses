# 설문 응답 예측 모델 개발

분류 문제 | 평가 기준: Accuracy | 분반 기준 최종 2위

## 1. 프로젝트 개요
- 목표: 패널 정보 및 설문 데이터를 기반으로 응답 여부(STATUS)를 예측하는 머신러닝 모델 개발
- 데이터 구성:  
  - train.csv (813,650건)  
  - test.csv (541,867건)  
  - 총 44개 컬럼 (패널 속성, 설문 정보, 응답 여부 포함)

## 2. 데이터 전처리 및 피처 엔지니어링
- 결측치 처리:  
  - 범주형 변수 → 최빈값  
  - 수치형 변수 → 평균값  
- 불필요 컬럼 제거:  
  - 결측률 30% 이상 컬럼  
  - 공백 등으로 인해 동일 값이 다르게 분류된 경우 정리

- 주요 피처 설계:
  - 응답률 기반 변수: 패널별 응답률, 설문별 응답률, 패널 타입별 응답률  
  - 설문 제목 키워드 포함 여부 (예: ‘해외’, ‘소비자’ 등)
  - 리워드 / 설문 시간 / 난이도 기반 파생 피처
  - 패널 성향 반영: 응답 횟수(Log 변환), 평균 리워드 점수 등

- 피처 변환:
  - 수치형: StandardScaler  
  - 범주형: Ordinal Encoding  

## 3. 모델링 및 성능 개선
- 모델 실험:  
  - 초기: DecisionTree, RandomForest 등  
  - 최종: LightGBM, CatBoost, XGBoost 조합

- 추가 전처리:
  - 응답률 변수는 소수점 아래 미세한 차이(예: 0.59992 vs 0.59993)로 인해 동일한 성향의 패널이 서로 다른 값으로 분류되는 문제 발생  
  - 모델 일반화를 위해 소수점 3자리에서 round 처리하여 구간화함

- Feature Selection:
  - SHAP 기반 중요도 분석
  - 중요도가 낮은 피처 제거, 성능 향상 피처 추가

- 하이퍼파라미터 튜닝:
  - 주요 파라미터(n_estimators, learning_rate, depth 등)는 반복 실험을 통해 수동 조정
  - CatBoost는 early stopping을 통해 과적합 방지, LightGBM과 XGBoost는 validation score 기준으로 조정

- 앙상블 방식:
  - 5-Fold OOF 교차검증을 기반으로 예측값을 생성하고 Soft Voting을 적용
  - threshold는 0.48~0.52 범위에서 실험한 결과, 0.495에서 가장 안정적인 성능을 보여 최종 적용
  - 이후 입력 데이터(X)에 대해 log, sqrt, MinMax 정규화를 각각 적용하고, 동일한 5-Fold OOF 방식으로 모델을 학습
  - 각 변환 방식별 예측값을 수집한 뒤, 최종적으로 하드 보팅(Hard Voting)을 통해 예측값을 결정



## 4. 결과 및 요약
응답률 기반 피처가 모델 성능 향상에 가장 크게 기여했다.  
추가한 피처가 과적합을 유발하지 않는지를 확인하기 위해, 학습 성능뿐 아니라 submission 결과까지 함께 비교하며 검증을 반복했다.

특히 응답률 변수는 target과 직접적으로 연결되는 민감한 파생 변수였기 때문에, 정밀도에 따라 모델이 지나치게 과적합되는 경향이 있었다.  
이런 문제를 완화하고 일반화 가능성을 높이기 위해, 소수점 3자리 기준으로 round 처리해 구간화하여 사용했다.

Soft Voting 기반 앙상블은 성능과 안정성 측면에서 효과적이었고, 다양한 데이터 변환을 적용한 학습 결과를 통합함으로써 일정 수준 이상의 예측력을 확보할 수 있었다.

모델 구조는 다소 복잡했지만, 데이터 양이 충분하고 클래스 불균형이 크지 않았기 때문에 이 방식이 잘 작동했던 것 같다.  
다만 이후 프로젝트에서는 데이터 크기나 분포에 따라 접근 방식이 달라져야 한다는 점을 체감하게 되었다
처음 경험한 성능 기반 공모전이었고, 다양한 시도와 실험을 통해 얻은 인사이트가 많았던 프로젝트였다.

## 5. 파일 안내
- `MLcompetition.ipynb`: 전체 실험 코드 및 결과 포함
- 세부 실험 로그 및 회고는 별도 노션 문서에 정리됨
